<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title></title>
    <description></description>
    <link>/</link>
    <atom:link href="/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Thu, 16 Dec 2021 14:00:25 +0000</pubDate>
    <lastBuildDate>Thu, 16 Dec 2021 14:00:25 +0000</lastBuildDate>
    <generator>Jekyll v4.1.1</generator>
    
      <item>
        <title>記一次HDFS_DELEGATION_TOKEN過期的問題</title>
        <description>&lt;p&gt;TL;DR&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-9276&quot;&gt;HDFS-9276&lt;/a&gt;, &lt;a href=&quot;https://github.com/apache/spark/pull/9168&quot;&gt;SPARK-11182&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;最近開發的Spark Streaming串流程式每七天就會報下面這串 Exception，起初也不以為意覺得是Hadoop叢集的問題，
後來觀察日誌才發現它是每七天就會出現一次，讓程式 crash 掉的問題。可以理解為，每七天Spark的HDFS_DELEGATION_TOKEN
就會失效，也就聯想到Hadoop叢集的 namenode delegation token 的最大lifetime的初始值也是7天。
(dfs.namenode.delegation.token.max-lifetime=604800000ms) and dfs.namenode.delegation.token.renew-
interval=86400000(1 day)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-angular2html&quot;&gt;ERROR JobScheduler: Error running job streaming job 1528366650000 ms.0
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager$InvalidToken):
token (token for spark_online: HDFS_DELEGATION_TOKEN owner=spark@XXXXX, renewer=yarn, realUser=, issueDate=1528232733578, maxDate=1528837533578, sequenceNumber=14567778, masterKeyId=1397)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;根據不管是 &lt;a href=&quot;https://spark.apache.org/docs/2.4.6/security.html#long-running-applications&quot;&gt;Spark 2.4.6&lt;/a&gt; 或者 
&lt;a href=&quot;https://spark.apache.org/docs/3.0.0-preview/security.html#long-running-applications&quot;&gt;Spark 3.0.0&lt;/a&gt; 以上的官方介紹文檔，
都表示Spark本身就可以根據使用者提供的keytab和principle去自動的renew token，關鍵句子如下。&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;Spark supports automatically creating new tokens for these applications when running in YARN mode. 
Kerberos credentials need to be provided to the Spark application via the spark-submit command, using the --principal and --keytab parameters.
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;但是，HDFS-9276說明，在開啟&lt;a href=&quot;https://codertw.com/程式語言/442243/&quot;&gt;高可用機制的 Hadoop 叢集&lt;/a&gt;上，緩存的 DFSClient 會使用舊的 hdfs_delegation_token 的，從而導致 token 在其最大的生命週期
到達時過期。就是 Spark 在更新 Hadoop HA 叢集的 delegation token 的時候，DFSClient 不會一起更新它的 token (
namenode token)。reference: &lt;a href=&quot;https://www.codenong.com/cs106192404/&quot;&gt;sparkThriftserver 长时间运行HDFS_DELEGATION_TOKEN失效问题&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;解決方法根據 &lt;a href=&quot;https://github.com/apache/spark/pull/9168&quot;&gt;SPARK-11182&lt;/a&gt; 裡面提到的， 可能的方式：&lt;/p&gt;

&lt;p&gt;(1) 更新 Hadoop 的版本到 2.9.0 或以上。（然而我們的版本是 2.6.0，更新的可能性不大）&lt;/p&gt;

&lt;p&gt;(2) 調整dfs.namenode.delegation.token.max-lifetime 等於一個較大的值，使得 token 最大生命週期變長，從而滿足使用者需要。&lt;/p&gt;

&lt;p&gt;(3) 調整設定 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;spark.hadoop.fs.hdfs.impl.disable.cache=true&lt;/code&gt;。然而，好像沒有效果。&lt;/p&gt;

&lt;p&gt;目前採用的方式是修改 Hadoop Cluster 上面的 token 的 max-lifetime，並在 token 生命週期到達的時候，排程重啟 Spark 
Structured Streaming 程式以更新 token。&lt;/p&gt;

&lt;p&gt;Reference&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://blog.csdn.net/u012150370/article/details/86518366&quot;&gt;记一次HDFS Delegation Token失效问题&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.jianshu.com/p/2904334ae404&quot;&gt;hdfs delegation token 过期问题分析&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.athemaster.com/resources/47&quot;&gt;解釋 Hadoop Delegation Token 上篇&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Thu, 09 Dec 2021 00:00:00 +0000</pubDate>
        <link>/%E6%8A%80%E8%A1%93/2021/12/09/%E8%A8%98%E4%B8%80%E6%AC%A1HDFS_DELEGATION_TOKEN%E9%81%8E%E6%9C%9F%E7%9A%84%E5%95%8F%E9%A1%8C/</link>
        <guid isPermaLink="true">/%E6%8A%80%E8%A1%93/2021/12/09/%E8%A8%98%E4%B8%80%E6%AC%A1HDFS_DELEGATION_TOKEN%E9%81%8E%E6%9C%9F%E7%9A%84%E5%95%8F%E9%A1%8C/</guid>
        
        
        <category>技術</category>
        
      </item>
    
      <item>
        <title>Spark Application with Kerberos - Intro</title>
        <description>&lt;p&gt;Kerberos 是三方認證機制，用戶和服務依賴於第三方（Kerberos 伺服器, KDC）來對彼此進行身份驗證。KDC 包含了一個認證
服務器 (Authenticate Server)，一個票證授權服務器 (Ticket-Granting Server)，以及一個記錄他所知道的 principles
和它們的 Kerberos 密碼的內建資料庫。&lt;/p&gt;

&lt;h4 id=&quot;principle&quot;&gt;Principle&lt;/h4&gt;

&lt;p&gt;一個 Kerberos principle 長得像，&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;username/fully.qualified.domain.name@YOUR-REALM.COM，按照先後順序，
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;它們分別是 primary，instance(option) 和 realm。primary 代表 Kerberos 的用戶，它可以是一個 unix account
比如 ap 帳號，也可以是Hadoop服務的Unix帳號，比如 hdfs。instance 是用來區分單個 user 的多個 principle 時使用。
realm 類似 DNS 中的域，不同的是 DNS 是定義了一組主機名，而 realm 則是定義了一組相關的 principle。&lt;/p&gt;

&lt;h4 id=&quot;keytab&quot;&gt;Keytab&lt;/h4&gt;

&lt;p&gt;Keytab 包含了加密的principle鍵值和對應的 Kerberos principle。 它被用來在驗證一個 Kerberos principle，
而不用人為得敲打密碼或者明碼保存密碼了。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-angular2html&quot;&gt;A keytab is a file containing pairs of Kerberos principals and encrypted keys (which are derived from the Kerberos password). 
You can use a keytab file to authenticate to various remote systems using Kerberos without entering a password.
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&quot;ticket-granting-ticket-tgt&quot;&gt;Ticket-Granting ticket TGT&lt;/h4&gt;
&lt;p&gt;由 KDC 中的 Authenticate Server頒發的票證，使用者帶著這個票證可以向 Server 端請求相應的服務。&lt;/p&gt;

&lt;h4 id=&quot;ccache-credential-cache&quot;&gt;ccache (Credential Cache)&lt;/h4&gt;
&lt;p&gt;Credential Cache 也是 Kerberos 驗證身份的一種方式，它持有Kerberos的憑證，使得使用者所執行任務session還有效時，
多次驗證服務器，而不用一直請求 KDC 服務器。&lt;/p&gt;

&lt;h3 id=&quot;long-running-spark-application-with-kerberos&quot;&gt;Long-running Spark Application with Kerberos&lt;/h3&gt;

&lt;p&gt;對於一個 Spark Streaming 服務，只要在 spark submit 指令後面加上 –keytab 和 –principle 兩個參數，
Spark 就會幫忙在生命週期終止前，接續的更新 hdfs delegation token 和 login ticket。&lt;/p&gt;

&lt;h3 id=&quot;kafka-with-a-kerberos-ticket-cache&quot;&gt;Kafka with a Kerberos Ticket Cache&lt;/h3&gt;

&lt;p&gt;如果你的 Spark Streaming 任務的串流資料來自於 Kafka，別忘了在 worker node 上面，由於 kafka 的 Kerberos 認證 
也會需要的 keytab 會需要送到 worker node 上面，否則會認證失敗。&lt;/p&gt;

&lt;p&gt;jaas_driver.conf&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;KafkaClient {
    com.sun.security.auth.module.Krb5LoginModule required
    useKeyTab=true
    storeKey=true
    keyTab=&quot;/path/on/driver/for/keytab&quot;
    principal=&quot;user@your.customized.realm.name&quot;
    debug=true
    serviceName=&quot;kafka&quot;;
};
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Reference&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://spark.apache.org/docs/2.4.6/running-on-yarn.html#yarn-specific-kerberos-configuration&quot;&gt;Running Spark on YARN&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Sat, 04 Dec 2021 00:00:00 +0000</pubDate>
        <link>/%E6%8A%80%E8%A1%93/2021/12/04/Spark-application-with-Kerberos/</link>
        <guid isPermaLink="true">/%E6%8A%80%E8%A1%93/2021/12/04/Spark-application-with-Kerberos/</guid>
        
        
        <category>技術</category>
        
      </item>
    
      <item>
        <title>kafka 小白二三事（二）</title>
        <description>&lt;p&gt;通常 Kafka 集群都會和 ZooKeeper 綁定，ZooKeeper 作為一個分佈式資源的集中服務扮演者協調 Kafka Server 的角色，
每個 Kafka broker 都會到 ZooKeeper 註冊。&lt;/p&gt;

&lt;p&gt;建置 Kafka 環境和相關設定&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;選擇版本並從官網安裝 https://downloads.apache.org/kafka/&lt;/li&gt;
  &lt;li&gt;設定 listeners 服務器真正 bind 的地址和 advertised.listeners，曝露給外部的 ip 地址，如果沒有設定的話會
直接使用 listeners 。他們告訴 Kafka 的客戶端要用什麼協議，哪個 ip 地址去訪問 Kafka 服務。&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&quot;language-angular2html&quot;&gt;e.g. 
listeners=PLAINTEXT://HOSTNAME:PORT,SSL://HOSTNAME:PORT

listeners=INSIDE://HOSTNAME:PORT,OUTSIDE://HOSTNAME:PORT
listener.security.protocol.map=INSIDE:SASL_PLAINTEXT,OUTSIDE:SASL_PLAINTEXT

P.S 
PLAINTEXT 代表 listeners 會是不需要授權或者加密的。
SASL_PLAINTEXT 代表需要 SASL 授權，但是不加密
SASL_SSL 代表 SASL 授權且使用 SSL 加密通訊
SSL 使用 SSL 加密通訊
&lt;/code&gt;&lt;/pre&gt;
</description>
        <pubDate>Tue, 30 Nov 2021 00:00:00 +0000</pubDate>
        <link>/%E6%8A%80%E8%A1%93/2021/11/30/kafka-%E5%B0%8F%E7%99%BD%E4%BA%8C%E4%B8%89%E4%BA%8B-%E4%BA%8C/</link>
        <guid isPermaLink="true">/%E6%8A%80%E8%A1%93/2021/11/30/kafka-%E5%B0%8F%E7%99%BD%E4%BA%8C%E4%B8%89%E4%BA%8B-%E4%BA%8C/</guid>
        
        
        <category>技術</category>
        
      </item>
    
      <item>
        <title>kafka 小白二三事（一）</title>
        <description>&lt;p&gt;Kafka，分佈式、高吞吐量、高擴展性消息隊列系統。是一個被廣為使用的消息隊列中間件。常用來異步處理請求、
流量削峰、日誌處理、解耦業務處理過程。。等等。&lt;/p&gt;

&lt;p&gt;生產消息 Producer，消費者 Consumer, Consumer Group，兩端業務邏輯被解耦一個負責生產數據，另一個就是單純
把數據接收下來，並且由 Consumer Group去記錄對應某個 Topic 目前信息接收到第幾條了 (offset)。這些生產者和
消費者彼此之間除了約定 Schema 之外沒有直接的關聯，生產者和消費者的吃飯的傢伙事活動都由 kafka broker 負責。
可以說一個 kafka broker 就是一個 kafka ，多個 kafka broker 就會組成一個 kafka 集群。&lt;/p&gt;

&lt;p&gt;名詞解釋&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Event&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;每個在 Kafka 內保存的資料可以視為一個事件，一個事件包含了鍵 key，值 value，時間戳 timestamp。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Topic&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;區分不同消息的放置位置和，好比 table name。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Partition&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;分區，這裡的概念基本上和 ElasticSearch 上的分片概念是一樣的。都是想把數據合理的放在不同 server 的分區上面，以達到
負載均衡。特別要注意的是，每個分區存儲的數據都是有序的，但是不同分區間的數據不保證有序性。若有保證有序，
那就只能設置一個分區。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Replica&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;副本，kafka 為了防止服務器壞掉採取的措施。需要注意的是不同分區的備份存放的是不同部分的數據，也就是如果沒有備份整個
kafka 集群就無法備份 kafka topic 的所有的分區。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Broker&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Broker 指運行 Kafka 的機器。多個 Broker 就組成 Kafka 集群。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Controller (Leader Broker)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Kafka controller 是一個 Kafka集群的頭腦部分，一個Kafka集群只能有一個 Controller，那個節點就和一般的broker無異，
只是多負責管理整個集群中所有分區和副本的狀態，比如 broker server 的上線、下線處理，創建/刪除 topic，處理每個 topic 的分區副本分配，以及 leader 
的選舉，等等。&lt;/p&gt;

&lt;p&gt;提到 Controller 就感覺還會需要科普學習一下下 zookeeper。官方文件寫到，&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services. &lt;/code&gt;
至少在 2021 年之前他對於 kafka都扮演重要的角色。 Kafka 保存了很多重要的元數據在 zookeeper，比如 kafka topic 的基本資訊，分區、副本等資訊，以及 broker 信息和ISR（In-Sync Replicas）等等。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ISR (In-Sync-replicas)&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Tue, 23 Nov 2021 00:00:00 +0000</pubDate>
        <link>/%E6%8A%80%E8%A1%93/2021/11/23/kafka-%E5%B0%8F%E7%99%BD%E4%BA%8C%E4%B8%89%E4%BA%8B-%E4%B8%80/</link>
        <guid isPermaLink="true">/%E6%8A%80%E8%A1%93/2021/11/23/kafka-%E5%B0%8F%E7%99%BD%E4%BA%8C%E4%B8%89%E4%BA%8B-%E4%B8%80/</guid>
        
        
        <category>技術</category>
        
      </item>
    
      <item>
        <title>踩坑 Azure TFS 部署使用者定義參數</title>
        <description>&lt;h3 id=&quot;前情提要&quot;&gt;前情提要&lt;/h3&gt;

&lt;p&gt;最近在角落研究微軟 Azure TFS 的 Release Pipeline 的參數傳遞的部分，Windows 的 Server 的 Release Pipeline 要讀取部署
Pipeline 上設置的環境變數，到 Linux Server 上面的 shell 做使用。每次覺得 code review 都一切 Look Very Good to
me 的時候，總是會有未知的彩蛋。在 Azure TFS 上設置的環境參數名稱，原本小寫的部分被轉成了大寫，不止如此參數名稱如果原本有帶 dot
的，也轉換成了底線。&lt;/p&gt;

&lt;h3 id=&quot;tfs-release-pipeline-參數名稱命名規範&quot;&gt;TFS Release Pipeline 參數名稱命名規範&lt;/h3&gt;

&lt;p&gt;因為需要把Azure TFS上設置的環境變量，直接帶到 Linux Server 上使用，不想一個一個參數的傳進 shell 檔裡面，參數多的時候真的會覺得
懷疑人生，所以稍微看了一下 Azure Pipeline 的使用者定義參數使用方式。
如果你也想讀英文，這是參考的&lt;a href=&quot;https://docs.microsoft.com/en-us/azure/devops/pipelines/process/variables?view=azure-devops&amp;amp;tabs=yaml%2Cbatch&amp;amp;WT.mc_id=DT-MVP-4015686#variable-naming-restrictions&quot;&gt;連結&lt;/a&gt;。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Macro Syntax Variables&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&quot;language-angular2html&quot;&gt;# 注意這邊是括號不是大括號
echo $(my_variable)
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
  &lt;li&gt;Template Expression Variables&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;範本參數，顧名思義會在編譯的時候就把參數做替換。一般來說，這個參數表達方式，會用在複用部署 YAML 檔的某一部分，並傳遞進去相應的參數。
當發現無法替換的參數，會直接替換成空字串。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Runtime Expression Variables&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;一般來說，這個參數表達方式，會用在條件式判斷去執行不同 Task 的時候。另外，它也只會在作為值的時候被使用，無法作為key值做使用。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-angular2html&quot;&gt;$[variables.key] : value # invalid
key : $[variables.value] # valid
key : $[variables.value]foo # invalid
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;release-pipeline-inline-script-命令行指令的保護措施&quot;&gt;Release Pipeline inline script 命令行指令的保護措施&lt;/h3&gt;

&lt;p&gt;當執行某些重要的指令，比如，登入會使用到的帳號密碼作為參數是不可缺少的部分，如果其中作為密碼的參數不見，很有可能會導致整個 Server 上因為 key 錯
密碼而發生災難。又或者是，在部署期間需要把某個路徑中的文件刪除。如果這個路徑使用環境參數拼出來的話，就會需要特別注意了。一個處理方式是，利用下面的 linux 條件
判斷式，排除某些重要參數為空值的情況。&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-z&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;password&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;+x&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-z&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;account&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;+x&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-z&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;/to/your/app+x&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;then
  &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Parameter Error!&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;exit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;fi&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;或者也有微軟 powershell 的寫法可以參考，&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;PUSH_ACCOUNT&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-And&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;PUSH_PASSWORD&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;# some very important command you want to execute&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
  Write-Host &lt;span class=&quot;s2&quot;&gt;&quot;Fatal. Missing Necessary Parameter&quot;&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
</description>
        <pubDate>Wed, 17 Nov 2021 00:00:00 +0000</pubDate>
        <link>/%E6%8A%80%E8%A1%93/2021/11/17/%E8%B8%A9%E5%9D%91-Azure-TFS-%E9%83%A8%E7%BD%B2%E4%BD%BF%E7%94%A8%E8%80%85%E8%87%AA%E5%AE%9A%E7%BE%A9%E5%8F%83%E6%95%B8/</link>
        <guid isPermaLink="true">/%E6%8A%80%E8%A1%93/2021/11/17/%E8%B8%A9%E5%9D%91-Azure-TFS-%E9%83%A8%E7%BD%B2%E4%BD%BF%E7%94%A8%E8%80%85%E8%87%AA%E5%AE%9A%E7%BE%A9%E5%8F%83%E6%95%B8/</guid>
        
        
        <category>技術</category>
        
      </item>
    
      <item>
        <title>ELK 監控串流 + Spark Structured Streaming</title>
        <description>&lt;p&gt;之前的做法是，串流數據ETL的結果，寫到Kafka的同時也寫一份到ElasticSearch，這樣就可以在Kibana上面分析串流ETL的效能，但是這樣需要記錄串流數據每一段
的時間戳，才能分析各段即時ETL的耗時。如果系統比較複雜，ETL數據串到最後會有一坨喇股的時間戳，只為了後面的Kibana分析使用。對中間的串流ETL而言都會是多餘的
資源消耗。&lt;/p&gt;

&lt;p&gt;目前讓 logstash 收集各段串流ETL的Kafka topic中的資料，並記錄 CreateTime。再根據event_id在 logstash 進行聚合，
就能計算出平均一筆event，整個串流ETL pipeline所需要的時間。上述是比較細的計算方式，如果只需要簡單的知道每段串流ETL
的效能，也可以只計算TPS(Transaction per seconds)，就是這段pipeline平均每秒處理了多少資料。&lt;/p&gt;

&lt;p&gt;TPS不能真的反應即時串流ETL延遲的情況，也可以理解為一個batch所需要的最小的時間，會是這個即時ETL的最低延遲。
如下說明，如果ETL串流的 Input Rate和Process Rate能保持差不多的速度，表示串流的處理資源足夠。但是，還是會有
因為一個 batch處理了太多的數據，而發生高延遲。解決的方法，是在縮小每個batch的大小和增加更多平行計算的資源，比如更多的
之間 Kafka partitions 或者更多的 Spark CPU core，之間找到平衡點。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Spark Structured Streaming&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Spark Structured Streaming 是一個分佈式即時串流數據處理框架。他使用了 Spark 的 SQL/DataSet/DataFrame API，
開發的人可以輕鬆的實現即時數據的聚合操作和 join 操作，還有窗口化 windowing。&lt;/p&gt;

&lt;p&gt;此外 Spark 3.0 開始，Structured Streaming 提供了一個監控串流ETL的監控 UI。可以清楚地看到，ETL串流的 Input Rate，
Process Rate 等。&lt;/p&gt;
</description>
        <pubDate>Fri, 12 Nov 2021 00:00:00 +0000</pubDate>
        <link>/%E6%8A%80%E8%A1%93/2021/11/12/ELK-%E7%9B%A3%E6%8E%A7%E4%B8%B2%E6%B5%81%E6%95%B8%E6%93%9A/</link>
        <guid isPermaLink="true">/%E6%8A%80%E8%A1%93/2021/11/12/ELK-%E7%9B%A3%E6%8E%A7%E4%B8%B2%E6%B5%81%E6%95%B8%E6%93%9A/</guid>
        
        
        <category>技術</category>
        
      </item>
    
      <item>
        <title>關於提升覺察力，真正接納自我</title>
        <description>&lt;p&gt;好吧，簡單來說上面標題的更白話一點的意思應該是，”慢慢卸下面具，放下想象中的模樣，開始與真正的自己連結。”&lt;/p&gt;

&lt;p&gt;進入職場、大人的世界之後，每天總是會為了迎合他人，而假裝地嘴角上揚，原本你自在的心還會一次次的提醒自己，”你好像已經很久沒有發自內心的笑了”。
直到假裝和表面變成反射動作，你開始對自己不包容、不開放，開始自我批判，下意識地與同儕比較和競爭，怕所有人背著你學習，怕自己不夠好、不夠厲害，甚至害怕
自己沒有別人想像中的那般厲害，於是不自信的思維從心底油然而生，最後誰也無法看清你是誰。接納自我：&lt;/p&gt;

&lt;p&gt;第一步，停止道歉和自我批判。最近工作上和同事聊到想在工作上”做自己”，而不是工作中總是在在”服務他人”，當然如果你從事的是服務業好像就是另當別論。
盤點下來，”做自己”的例子比如，大聲區分工作權責，或者大聲說自己真的想做什麼，而不是不帶感情的完成一件待辦事項。當然這和你富有熱情、熱忱地，想要幫忙解決問題的
情況是完全不同的。熱忱如果不是出自真正接納自己恐會是淪為三分鐘熱度，又或者只是你的其中一個面具。&lt;/p&gt;

&lt;p&gt;第二步，回想自信時刻。自信和自負的區別，打個比方來說就是，你抬起的是下巴還是鼻子，就是你能否接受來自他人的意見或批評。
避免自己在嘗試做自己的時候，迷失方向，看不到其他人，而逃避了其他人，最後在自我逃避。&lt;/p&gt;

&lt;p&gt;第三步，採取行動。找到自己最自在的那個時候，從那開始跨出舒適圈，讓自己去挑戰或爭取心中渴望已久的事物，並將實踐的過程與理想的成果形象化。
清楚地找到自己的目標。&lt;/p&gt;

&lt;p&gt;覺察力，讓你更了解自己，也更容易接納他人，更能夠與他人連結。&lt;/p&gt;
</description>
        <pubDate>Tue, 09 Nov 2021 00:00:00 +0000</pubDate>
        <link>/%E6%9B%B8%E5%B1%8B/2021/11/09/%E9%97%9C%E6%96%BC%E6%8F%90%E5%8D%87%E8%A6%BA%E5%AF%9F%E5%8A%9B-%E7%9C%9F%E6%AD%A3%E6%8E%A5%E7%B4%8D%E8%87%AA%E6%88%91/</link>
        <guid isPermaLink="true">/%E6%9B%B8%E5%B1%8B/2021/11/09/%E9%97%9C%E6%96%BC%E6%8F%90%E5%8D%87%E8%A6%BA%E5%AF%9F%E5%8A%9B-%E7%9C%9F%E6%AD%A3%E6%8E%A5%E7%B4%8D%E8%87%AA%E6%88%91/</guid>
        
        
        <category>書屋</category>
        
      </item>
    
      <item>
        <title>ELK 小白記錄</title>
        <description>&lt;p&gt;ELK是ElasticSearch， Logstash， Kibana三個名詞合在一起的縮寫。 通常用於日誌管理，從集群中的多個service收集日誌，避免開發者直接登入server，
使得生產環境的安全和查找問題的便利性。&lt;/p&gt;

&lt;p&gt;其中，ElasticSearch提供日誌保存，檢索和分析的功能，基於 Apache 
Lucene 真的就是未名覺厲。Logstash收集log並做一些前處理，除了將一些常用的反序列數據的功能變得很好上手，如常見的json，csv。更強大的是， 
grok， dissect 等插件，將非結構化的字符串轉換成結構化的數據， 當然也可以添加或刪除欄位 add_field， 
remove_field。Kibana 則提供資料視覺化的dashboard，直觀的 Kibana Query Language，讓使用者可以更快知道資料的全貌。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Beat and Logstash&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Beats 是一個小型的日誌發送程序的集合，大家最常使用的就是 Filebeat，它能夠將日誌發送到 ELK 集群。除此之外，Beat家族還包括 Packetbeat, 
Metricbeat，Heartbeat，Functionbeat … 等。一般在將日誌文件導入到ElasticSearch進行檢索時，會同時使用 
Filebeat和Logstash，Filebeat負責將服務器上的日誌發送到ELK集群，Logstash則負責將日誌數據轉換成我們需要的格式。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ElasticSearch&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ElasticSearch 和 MongoDB一樣都是NoSQL的資料庫，所有的資料都以JSON格式讀取。不同於傳統的關係型資料庫，文檔和文檔之間的關係會使用 
join來串接，ElasticSearch 通常會用使用sub document的方式。 此外，ElasticSearch 對所有欄位都可以建立索引，並且可以根據不同的用途建立索引。&lt;/p&gt;

</description>
        <pubDate>Sat, 06 Nov 2021 00:00:00 +0000</pubDate>
        <link>/%E6%8A%80%E8%A1%93/2021/11/06/ELK-%E5%B0%8F%E7%99%BD%E8%A8%98%E9%8C%84/</link>
        <guid isPermaLink="true">/%E6%8A%80%E8%A1%93/2021/11/06/ELK-%E5%B0%8F%E7%99%BD%E8%A8%98%E9%8C%84/</guid>
        
        
        <category>技術</category>
        
      </item>
    
      <item>
        <title>logstash 初次見面</title>
        <description>&lt;p&gt;Logstash: Collect, Parse and Transform logs.&lt;/p&gt;

&lt;p&gt;Logstash 是一個用來做日誌收集，分析，過濾的工具。初見時
logstash的對資料流的設定檔案，名稱前面標示了數字，聽前說人檔案會按照 filename 字母順序組合，這在需要切分複雜設定的時候
或許有些用，也要特別注意用條件來區分不同的資料流。但使用下來我還是覺得如果沒有需要幾千行的設定，那還是放在一個設定檔裡面就好了。&lt;/p&gt;

&lt;h3 id=&quot;-設定檔格式&quot;&gt;# 設定檔格式&lt;/h3&gt;

&lt;p&gt;input, filter, output 的組成非常符合 Logstash 的人設。要特別說的是檔案裡面也是可以空行和寫注釋的，
畢竟只是一個定義資料流的腳本，寫明白資料的來源格式和指定輸出格式，以及解析的方式還是很重要的。&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;input { # comments can appear at the end of a line, too
  http {
    codec =&amp;gt; json {
      charset =&amp;gt; &quot;ISO-8859-1&quot;
      target =&amp;gt; &quot;[document]&quot;
    }
  }
}

filter {
}

output {
}
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;-常用的-plugin&quot;&gt;# 常用的 plugin&lt;/h3&gt;

&lt;h4 id=&quot;codec&quot;&gt;codec&lt;/h4&gt;

&lt;p&gt;codec 表示編解碼插件，可以按照約定好的格式將數據讀進來logstash。例如直接將json字符串格式的資料定義為key value的格式。
一些常見的編解碼插件如下，&lt;/p&gt;

&lt;p&gt;(1) json
(2) csv
(3) protobuf&lt;/p&gt;

&lt;p&gt;Google Protocol Buffer 是一種輕便高效的結構化存儲數據的格式。&lt;/p&gt;

&lt;p&gt;(4) multiline&lt;/p&gt;

&lt;p&gt;Collapse multiline messages and merge them into a single event. 例如，當你需要將任何一行不是以時間戳記
開頭的字符串合併到它的上一行，就可以很方便的使用這個 codec。官方給的實現方式如下，&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;    input {
      file {
        path =&amp;gt; &quot;/var/log/someapp.log&quot;
        codec =&amp;gt; multiline {
          # Grok pattern names are valid! :)
          pattern =&amp;gt; &quot;^%{TIMESTAMP_ISO8601} &quot;
          negate =&amp;gt; true
          what =&amp;gt; &quot;previous&quot;
        }
      }
    }
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;(5) rubydebug&lt;/p&gt;

&lt;p&gt;輸出時使用，通常會用在 debug 的時候。它能將logstash的事件用Ruby Amazing Print library的格式輸出。&lt;/p&gt;

&lt;h4 id=&quot;grok&quot;&gt;grok&lt;/h4&gt;

&lt;p&gt;grok 是一個 filter 插件，負責把收到的數據，以使用者自定義的正則處理，讓數據變成使用者想要的鍵值對的格式。&lt;/p&gt;

&lt;p&gt;特別的，由於日誌通常很長一串又格式複雜，於是 &lt;a href=&quot;https://grokdebug.herokuapp.com&quot;&gt;Grok Debugger Tool&lt;/a&gt; 就出現了，
可以直接透過這個小工具來檢查自己的解析語法是否match到所有自己想要的欄位。&lt;/p&gt;
</description>
        <pubDate>Fri, 15 Oct 2021 00:00:00 +0000</pubDate>
        <link>/%E6%8A%80%E8%A1%93/2021/10/15/logstash-%E5%88%9D%E6%AC%A1%E8%A6%8B%E9%9D%A2/</link>
        <guid isPermaLink="true">/%E6%8A%80%E8%A1%93/2021/10/15/logstash-%E5%88%9D%E6%AC%A1%E8%A6%8B%E9%9D%A2/</guid>
        
        
        <category>技術</category>
        
      </item>
    
      <item>
        <title>kafka connect 原來是你</title>
        <description>&lt;h3 id=&quot;-前情提要&quot;&gt;# 前情提要&lt;/h3&gt;

&lt;p&gt;應用場景需要將插入 Oracle 資料庫的資料，用串流的方式寫入 Kafka，Oracle 官方提供的解決方案是 OCIS，Oracle
Cloud Infrastructure Stream 裡面的 Kafka Connect Harness，沒有錯這是一個付費服務，估計是在原先開源 Kafka 
Connect 對 Oracle 的支持上進行了優化，如果口袋沒有很深去嘗試使用它，真的就只能好好調研一下了。是說這篇是要介紹
Kafka Connect的啦，趕快拉回正題。&lt;/p&gt;

&lt;p&gt;Kafka Connect 是一個介接其他數據庫和 Kafka 之間的串流工具，它繼承了 Kafka 的可擴展性(scalability)和高可靠性(reliably)
。它可以接收整個數據庫，比如 Oracle，或者收集特定的指標，送到指定的 Kafka topic。&lt;/p&gt;

&lt;h3 id=&quot;-source-connector-modes&quot;&gt;# Source Connector Modes&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;bulk modes&lt;/p&gt;

    &lt;p&gt;整張表每次都重新讀一遍。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Incremental Mode With Incrementing Column&lt;/p&gt;

    &lt;p&gt;用一個遞增的欄位做辨別，只讀取新的資料。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Incremental Mode With Timestamp Column&lt;/p&gt;

    &lt;p&gt;用一個時間欄位當做 Kafka Connect 抓取資料的依據，並假設資料更新會同時更新時間戳記。會讀取新的或者更新後的資料。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Incremental Mode With Incrementing and Timestamp Columns&lt;/p&gt;

    &lt;p&gt;每一行數據都可以被對應到一個唯一的串流 offset。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Fri, 08 Oct 2021 00:00:00 +0000</pubDate>
        <link>/%E6%8A%80%E8%A1%93/2021/10/08/kafka-connect-%E5%8E%9F%E4%BE%86%E6%98%AF%E4%BD%A0/</link>
        <guid isPermaLink="true">/%E6%8A%80%E8%A1%93/2021/10/08/kafka-connect-%E5%8E%9F%E4%BE%86%E6%98%AF%E4%BD%A0/</guid>
        
        
        <category>技術</category>
        
      </item>
    
  </channel>
</rss>
