---
layout: post
title: "Spark你應該要會的知識"
subtitle: ""
date: 2021-09-11
categories: [技術]
---

Apache Spark是為大數據處理而設計的通用分散式計算引擎，可以用來開發大型的低延遲資料分析應用程式。
它從一開始的設計原則就是為了解決 MapReduce 的不足。
MapReduce 這個計算框架雖然已經得到廣泛的接受，但是還是有很多它不適合的應用場景。比如機器學習中常見的迭代計算方法，
或者對實時性要求較高的串流數據處理應用。於是 Spark 在 2009 年誕生，提出來一種記憶體中的彈性分佈式資料結構 RDD，
一步步發展優化比他大5歲的 MapReduce 的不足。

## Table of Spark

本文主要介紹關於 Spark 的基礎知識，包括如下幾個方面。Spark 歷史，側重介紹 Spark 各版本有什麼新功能。以及 Spark 的
各種 Libraries。還有一個 Spark 應用程式的各種任務執行方式。

1. Spark 歷史
2. Spark Libraries
3. Spark Application 運行方式
4. Spark 和 Hadoop 的比較

## Spark 歷史

2009年 Spark 誕生於柏克萊大學的AMPLab，在2014年 Spark 成為 Apache 的頂級項目，當時 Spark 版本 1.0.0。
2016年5月，Spark 版本2.0發佈，比較大的更新有：

1）引入 SparkSession 的概念，提供使用者一致的接口去使用 Spark 的各項功能，統一了舊的 SQLContext 與 HiveContext。

2）統一 DataFrame 和 DataSet API，把 DataFrame 當做特殊的 DataSet。

3）在 Spark Streaming 基礎上結合 SparkSQL 加入 High-Level API，Spark Structured Streaming。

2020年6月，Spark 3.0發佈，新特性包括自适应查询执行、動態分區裁剪、Join Hints等性能優化，此外也優化了pandas API，
以及加入 Structured Streaming 的新 UI 界面。有好多新功能都值得深入研究。

## Spark Libraries

**# Spark Core 裡面包含了 RDD 分佈式數據結構、任務調度、內存管理**

RDD(Resilient Distributed Dataset) 是一種彈性分佈式數據集合。他可以由數據源產生，或者通過一個 RDD 轉換成另外一個。
RDD有三個特性：

1）不可變的(Immutable)。這是 RDD 可以用於分佈式儲存的重要特性。

2）彈性(Resilient)，如果叢集有節點掛掉，Spark 會根據 RDD Lineage 自動重建RDD。
每個 RDD 都記錄了自己是怎麼從持久化的儲存元數據計算出來的，也就是 RDD Lineage。

3）分佈式(Distributed)，RDD 可以跨多個節點，並預設把資料儲存在每個節點的記憶體內。除非資料量太龐大，才會選擇將設定成
 RDD 保存在磁盤上。

DAG(Directed Acyclic Graph) 有向無環圖，是一組頂點和邊的集合，在 Spark 中頂點表示 RDD 數據集合，邊表示對 RDD 的操作。
Spark 的 DAGScheduler 會根據 RDD 的 Transformation 操作將 DAG 分為不同的 Stage，每個 Stage 又被分為多個可以並行
執行的 task。

**# Spark SQL 是 Spark 用來處理結構化資料的模組。**

它將關係型資料處理操作和 Spark API 結合。官方網站上列了 SparkSQL 的特點包括：

1）Integrated, SparkSQL將對結構化的 SQL 查詢整合到 Spark 程式中，你可以方便的在 Python, Java, Scala, R 中使用
SQL 或者 DataFrame API 。

2）Uniform Data Access, 各式的資料源都可以用 SparkSQL 讀取。資料源如 Hive，Avro, Parquet, ORC, JSON, and JDBC等。

3）Hive integration，在資料倉儲上處理 SQL。SparkSQL 可以使用資料倉儲 Hive 的資料和 metastore，也包括 Hive 的SerDes
和 UDFs 接口。

4）Standard connectivity，SparkSQL 提供 JDBC 或 ODBC 等連線介面，連接商業智慧工具 (BI Tools)。

**# Spark Streaming, 這個模組用來構建可拓展的、可容錯的數據串流應用程式。**

**# Spark MLlib，是 Spark 的可拓展性能的機器學習模組。**
Spark 是基於記憶體計算的框架，MLlib 又包含了高效率的優化迭代算法，它的性能可以比 MapReduce 快 100 倍。

**# Spark GraphX，讓 Spark 支援圖計算的模組。**

## Spark Application 運行方式

Spark 的運行模式可以分為 client 模式和 cluster 模式: 

1）本地模式(local)，將 Spark 應用程式以多線程的方式在本地執行。

2）集群模式(cluster)
   - 獨立集群模式(standalone)，不依賴第三方集群資源管理器(cluster manager)
   - Spark on YARN，依賴 Hadoop YARN 
   - Spark on Mesos，依賴 Apache Mesos 
   - Spark on Kubernetes

以 yarn-client 模式為例，一個 Spark Application 啟動時，它的 driver 會在本地運行，建立 SparkContext 並向
ResourceManager 請求啟動自己的 Application Master。之後，Application Master 會向 ResourceManager 請求在集群中
申請 container 資源(executor)。Executor 會向 client 報告任務運行狀態。當所有任務計算完畢，client 向 ResourceManager
申請註銷和關閉自己。

以 yarn-cluster 模式為例，一個 Spark Application 啟動時，它的 driver 會運行在 nodeManager 上。ResourceManager
收到 client 的請求之後也是在集群中選擇 NodeManager 在其所管理的節點上分配 container 作為 ApplicationMaster。
隨後 ApplicationMaster 向 ResourceManager 申請資源。當所有任務計算完畢，ApplicationMaster 向 ResourceManager
申請註銷和關閉自己。


## Spark 和 Hadoop 的比較

Apache Spark 和 Apache Hadoop 有著可合可獨立使用的關係，脫離 Spark，Hadoop 原本就是一個大數據計算平台（獨立）。
但是 Spark 本身只是一個計算引擎，它需要和其他的分佈式文件系統合作才能運作，Hadoop HDFS 就是一個大家默認的選擇。(合)
前面有提到 Spark 是 MapReduce 的替代品，Spark 框架實現了一套記憶體版本的 MapReduce，Hadoop MapReduce 則是每進行
一次資料計算就將結果寫回磁盤。

*MapReduce 是一個分而治之的概念，把任務拆分成多個子任務各別完成後再結合所有子任務的結果。以 Word Count 計算詞頻的例子
說明，MapReduce 將要計算的詞頻任務分割成多個分片作為 map 階段子任務的輸入，再將子任務的輸出作為 Reduce 階段子任務的輸入。
然而 Map子任務和 Reduce子任務通常都會在 Hadoop 集群不同的節點上，需要跨節點去拉 Map 任務的結果，於是 shuffle 登場。
shuffle (洗牌)的一個目的是盡可能減少不必要的帶寬。*

回顧一下，Apache Hadoop 從2006年啟動，他的名字是以 Doug Cutting，Hadoop之父的兒子的玩具大象命名。
Hadoop是一個大數據分佈式儲存和計算的平台，它的長處是海量數據的計算，他起源於 Google 的三大論文，

1）GFS：Google 的分佈式文件系統 (Google File System)，Hadoop HDFS 的前身。

2）MapReduce：分佈式並行計算框架。分別統計詞頻 (Map) 再把所有人統計的數加在一起 (Reduce)，得到結果。

3）BigTable：大型分佈式數據庫


## Reference

1. [Spark 官方網站](https://spark.apache.org)
2. [Spark优化总结（一）——数据倾斜](https://blog.csdn.net/alionsss/article/details/103802315)
3. [Apache Spark 3.0.0重磅发布 —— 重要特性全面解析](https://developer.aliyun.com/article/765975)
4. [深入研究 Apache Spark 3.0 的新功能](https://blog.csdn.net/weixin_45906054/article/details/107948166)
5. [Spark News](https://spark.apache.org/news/index.html)
6. [spark1.x和spark2.x的區別](https://www.twblogs.net/a/5c9dae01bd9eee73ef4b2647)
7. [Spark--spark工作模式详解(local/standalone/yarn)](https://www.jianshu.com/p/464c2497870c)
