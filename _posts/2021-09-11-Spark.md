---
layout: post
title: "Spark你應該要會的知識"
subtitle: ""
date: 2021-09-11
categories: [技術]
---

Apache Spark是為大數據處理而設計的通用計算引擎，可以用來開發大型的低延遲資料分析應用。
Spark可以理解是 MapReduce 的替代品，它是一個分散式的計算框架。

## Table of Spark

1. Spark 歷史
2. Spark 和 Hadoop 的比較
3. Spark Libraries
4. Spark Application 運行方式
5. Spark 優化

## Spark 歷史

2009年 Spark 誕生於柏克萊大學的AMPLab，在2014年 Spark 成為 Apache 的頂級項目，當時 Spark 版本 1.0.0。
2016年5月，Spark 版本2.0發佈，比較大的更新有：

1）引入SparkSession 的概念，提供使用者一致的接口去使用 Spark 的各項功能，統一了舊的SQLContext與HiveContext。

2）統一 DataFrame 和 DataSet API，把 DataFrame 當做特殊的 DataSet。

3）在 Spark Streaming 基礎上結合 SparkSQL 加入 High-Level API，Spark Structured Streaming。

2020年6月，Spark 版本3.0發佈。新特性包括自适应查询执行、動態分區裁剪、Join Hints等性能優化，此外也優化了pandas API，
以及加入 Structured Streaming 的新 UI 界面。有好多新功能值得深入研究。

## Spark 和 Hadoop 的比較

Apache Hadoop 從2006年啟動，他的名字是以 Doug Cutting，Hadoop之父的兒子的玩具大象命名。
Hadoop是一個大數據分佈式儲存和計算的平台，他起源於 Google 的三大論文，

1）GFS：Google 的分佈式文件系統 (Google File System)，Hadoop HDFS 的前身。

2）MapReduce：分佈式並行計算框架。分別統計詞頻 (Map) 再把所有人統計的數加在一起 (Reduce)，得到結果。

3）BigTable：大型分佈式數據庫

前面有提到 Spark 是 MapReduce 的替代品，當然 Spark 和 Hadoop 有著可合可獨立使用的關係，脫離
Spark，Hadoop 原本就是一個大數據計算平台（分）。但是 Spark 本身只是一個通用計算引擎，它需要和其
他的分佈式文件系統合作才能運作，Hadoop HDFS 就是一個大家默認的選擇。(合)

Spark 會在記憶體完成計算，而 Hadoop MapReduce 則是每進行一次資料計算就將結果寫回磁盤，再要進行下一個
計算時又會從磁盤讀取資料。

https://www.modb.pro/db/169190

https://blog.tibame.com/?p=1752

## Spark Libraries

**# Spark Core 裡面包含了 RDD 分佈式數據結構、任務調度、內存管理**

RDD(Resilient Distributed Dataset) 是一種彈性分佈式數據集合。他可以由數據源產生，或者通過一個 RDD 轉換成另外一個。
RDD有三個特性：

1）不可變的(Immutable)。這是 RDD 可以用於分佈式儲存的重要特性。

2）彈性(Resilient)，如果叢集有節點掛掉，Spark 會根據 RDD Lineage 自動重建RDD。
每個 RDD 都記錄了自己是怎麼從持久化的儲存元數據計算出來的，也就是 RDD Lineage。

3）分佈式(Distributed)，RDD 可以跨多個節點，並預設把資料儲存在每個節點的記憶體內。除非資料量太龐大，才會選擇將設定成 RDD 保存在硬碟上。

DAG(Directed Acyclic Graph) 有向無環圖，是一組頂點和邊的集合，在 Spark 中頂點表示 RDD 數據集合，邊表示對 RDD 的操作。
Spark 的 DAGScheduler 會根據 RDD 的 Transformation 操作將 DAG 分為不同的 Stage，每個 Stage 又被分為多個可以並行
執行的 task。

**# Spark SQL 是 Spark 用來處理結構化資料的模組。**

它將關係型資料處理操作和 Spark API 結合。官方網站上列了 SparkSQL 的特點包括：

1）Integrated, SparkSQL將對結構化的 SQL 查詢整合到 Spark 程式中，你可以方便的在 Python, 
Java, Scala, R 中使用 SQL 或者 DataFrame API 。

2）Uniform Data Access, 各式的資料源都可以用 SparkSQL 讀取。資料源如 Hive，Avro, Parquet, ORC, JSON, and JDBC等。

3）Hive integration，在資料倉儲上處理 SQL。SparkSQL 可以使用資料倉儲 Hive 的資料和 metastore，也包括 Hive 的SerDes
和 UDFs 接口。

4）Standard connectivity，SparkSQL 提供 JDBC 或 ODBC 等連線介面，連接商業智慧工具 (BI Tools)。

**# Spark Streaming, 這個模組用來構建可拓展的、可容錯的數據串流應用程式。**

**# Spark MLlib，是 Spark 的可拓展性能的機器學習模組。**
Spark 是基於記憶體計算的框架，MLlib 又包含了高效率的優化迭代算法，它的性能可以比 MapReduce 快 100 倍。

**# Spark GraphX，讓 Spark 支援圖計算的模組。**

## Spark 工作方式

Spark 的運行模式可以分為 client 模式和 cluster 模式: 

1）本地模式(local)，將 Spark 應用程式以多線程的方式在本地執行。

2）集群模式(cluster)
   - 獨立集群模式(standalone)，不依賴第三方集群資源管理器(cluster manager)
   - Spark on YARN，依賴 Hadoop YARN 
   - Spark on Mesos，依賴 Apache Mesos 
   - Spark on Kubernetes

以 yarn-client 模式為例，一個 Spark Application 啟動時，它的 driver 會在本地運行，建立 SparkContext 並向
ResourceManager 請求啟動自己的 Application Master。之後，Application Master 會向 ResourceManager 請求在集群中
申請 container 資源(executor)。Executor 會向 client 報告任務運行狀態。當所有任務計算完畢，client 向 ResourceManager
申請註銷和關閉自己。

以 yarn-cluster 模式為例，一個 Spark Application 啟動時，它的 driver 會運行在 nodeManager 上。ResourceManager
收到 client 的請求之後也是在集群中選擇 NodeManager 在其所管理的節點上分配 container 作為 ApplicationMaster。
隨後 ApplicationMaster 向 ResourceManager 申請資源。當所有任務計算完畢，ApplicationMaster 向 ResourceManager
申請註銷和關閉自己。

## Spark 優化




## Reference

1. [Spark 官方網站](https://spark.apache.org)
2. [Spark优化总结（一）——数据倾斜](https://blog.csdn.net/alionsss/article/details/103802315)
3. [Apache Spark 3.0.0重磅发布 —— 重要特性全面解析](https://developer.aliyun.com/article/765975)
4. [深入研究 Apache Spark 3.0 的新功能](https://blog.csdn.net/weixin_45906054/article/details/107948166)
5. [Spark News](https://spark.apache.org/news/index.html)
6. [spark1.x和spark2.x的區別](https://www.twblogs.net/a/5c9dae01bd9eee73ef4b2647)
7. [Spark--spark工作模式详解(local/standalone/yarn)](https://www.jianshu.com/p/464c2497870c)
